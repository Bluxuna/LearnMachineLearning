# ğŸ“š K-Nearest Neighbors (KNN) from Scratch

This folder contains a **from-scratch** implementation of the **K-Nearest Neighbors (KNN)** algorithm for **classification** and **regression** using only **NumPy** and basic Python.

The goal is to help students **understand how KNN works internally** â€” without relying on high-level libraries â€” and apply it to **real-world datasets**.

---

## ğŸ” What is KNN?

K-Nearest Neighbors is a **supervised machine learning algorithm** used for:
- **Classification** â†’ Predicting a class label
- **Regression** â†’ Predicting a continuous value

Itâ€™s a **lazy learning algorithm**:  
- It doesnâ€™t learn a parametric model during training.
- Instead, it stores the training data and makes predictions by comparing new data points to the stored ones.

---

## âš™ï¸ How KNN Works (Step-by-Step)

### **1. Choose `k`**
- `k` = the number of neighbors to consider.
- Small `k` â†’ sensitive to noise.  
- Large `k` â†’ smoother decision boundaries but may underfit.

---

### **2. Calculate Distance**
For a new point `x`, compute the distance to every training point.

**Common choices:**

- **Euclidean distance** (default in our code):  
  `distance(x, y) = sqrt( Î£ (xáµ¢ - yáµ¢)Â² )`

- **Manhattan distance**:  
  `distance(x, y) = Î£ |xáµ¢ - yáµ¢|`

---

### **3. Find the K Nearest Neighbors**
- Sort the training points by distance.
- Take the top `k` closest points.

---

### **4. Make Prediction**
- **Classification** â†’ Majority vote among the k neighbors.  
- **Regression** â†’ Average the target values of the k neighbors.

---

### **5. Evaluate**
- Classification â†’ Accuracy, F1-score  
- Regression â†’ RMSE, MAE

---

## ğŸ§® Complexity
- **Training time**: \( O(1) \) (just store the data)
- **Prediction time**: \( O(n \cdot d) \) where:
  - `n` = number of training samples
  - `d` = number of features

---

## ğŸ“¦ Project Structure
